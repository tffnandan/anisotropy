# -*- coding: utf-8 -*-
"""ALLinONE_bidirectional.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11T4dyRipjAxMYu0S92KNCkE-IzHB2IDY
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import tensorflow as tf
from tensorflow.keras.layers import *
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.optimizers import Adam, RMSprop
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
from sklearn import metrics
from sklearn.metrics import f1_score
from keras.models import Sequential
import matplotlib.pyplot as plt
from keras.models import Model
# from keras.layers.normalization import BatchNormalization
from keras.layers.convolutional import Conv2D
from keras.layers.convolutional import MaxPooling2D
from keras.layers.core import Activation
from keras.layers.core import Dropout
from keras.layers.core import Lambda
from keras.layers.core import Dense
from keras.layers import Flatten
from keras.layers import Input
import tensorflow as tf
from keras.utils.vis_utils import plot_model
# from keras.utils import plot_model
from keras.layers import Dense
from sklearn import datasets, linear_model, metrics
from sklearn import datasets, linear_model
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
from sklearn.metrics import f1_score
from sklearn.model_selection import train_test_split # for splitting the data
from sklearn.metrics import mean_squared_error # for calculating the cost function
from sklearn.ensemble import RandomForestRegressor # for building the model
import os, sys

from google.colab import drive
drive.mount('/content/drive')
os.chdir("/content/drive/MyDrive/microscpoic model")

# To remove the scientific notation from numpy arrays
np.set_printoptions(suppress=True)

CF1=pd.read_excel('CF_model_data.xlsx','Sheet1')

CF1=CF1[CF1['following']==1]
CF2=CF1[CF1['long_acceleration']<=4]
CF= CF2[CF2['long_acceleration']>=-4]

#T/B-tw-T/B:[313,314,413,414,363,364,463,464]
#Car-tw-T/B:[213,214,263,264,513,514,563,564]
#Car-car-T/B:[223,224,523,524,553,554,253,254]
#T/B-car-T/B:[323,324,353,354,423,424,453,454]
#Car-car-car:[222,252,225,255,522,525,552,555]
#Car-car-tw:[221,226,251,256,521,526,551,556]
CF_data1=CF[CF["follow_type"].isin([222,252,225,255,522,525,552,555])]
CF_data2=CF_data1[CF_data1['headw_leader']<50]
CF_data=CF_data2[CF_data2['headw_follo']<50]

CF_data.head()

len(CF_data['sub_vx'])

plt.figure(figsize=(15,6)) 
plt.plot(np.arange(0,len(CF_data['sub_vx'][5:45]),1), CF_data['long_velocity'][5:45], 'o--', color='black', alpha=0.5,label='Filtered')
plt.plot(np.arange(0,len(CF1['sub_vx'][5:45]),1), CF1['long_velocity'][5:45], 'o--', color='green', alpha=0.5, label='unfiltered')
plt.ylabel('speed', fontsize=20)
plt.xlabel('time_steps', fontsize=20)
plt.xticks(fontsize=20)
plt.yticks(fontsize=20)
# plt.title("Multi-branch NN: follw")
plt.legend(fontsize=20)
# plt.savefig("data_pre.jpg", dpi=550)
plt.show()

len(CF_data)

# CF_data['veh_no'].unique()
# CF_data['veh_no'].unique()
# cf_train=CF_data[CF_data['veh_no'].isin([3047, 2206, 3363])]
# cf_test=CF_data[CF_data['veh_no'].isin([3025, 3286])]

#input1
TargetVariable=['sub_vx']
#for bidirectional
Predictors=['lead_vx','follo_vx','headw_leader', 'headw_follo']
#for leader
# Predictors=['lead_vx', 'headw_leader']

# Predictors=['lead_vx','follo_vx','lead_vy','follo_vy','headw_leader', 'headw_follo']
X=CF_data[Predictors]
y=CF_data[TargetVariable]



# splitting X and y into training and testing sets
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,
													random_state=42)


# X_train = cf_train[Predictors].values
# X_test = cf_test[Predictors].values
# y_train = cf_train[TargetVariable].values
# y_test = cf_test[TargetVariable].values

#########
#Linear Regression
###########
# create linear regression object
reg = linear_model.LinearRegression()

# train the model using the training sets
reg.fit(X_train, y_train)

# regression coefficients
print('Coefficients: ', reg.coef_)

# variance score: 1 means perfect prediction
print('Variance score: {}'.format(reg.score(X_test, y_test)))

# plot for residual error

# ## setting plot style
# plt.style.use('fivethirtyeight')

# ## plotting residual errors in training data
# plt.scatter(reg.predict(X_train), reg.predict(X_train) - y_train,
# 			color = "green", s = 10, label = 'Train data')

# ## plotting residual errors in test data
# plt.scatter(reg.predict(X_test), reg.predict(X_test) - y_test,
# 			color = "blue", s = 10, label = 'Test data')

# ## plotting line for zero residual error
# plt.hlines(y = 0, xmin = 0, xmax = 50, linewidth = 2)

# ## plotting legend
# plt.legend(loc = 'upper right')

# ## plot title
# plt.title("Residual errors")

# ## method call for showing the plot
# plt.show()

# The coefficients
print("Coefficients: \n", reg.coef_)
# The mean squared error
print("Mean squared error: %.2f" % mean_squared_error(y_test, reg.predict(X_test)))
# The coefficient of determination: 1 is perfect prediction
print("Coefficient of determination: %.2f" % r2_score(y_test, reg.predict(X_test)))

plt.figure(figsize=(15,6)) 
plt.plot(np.arange(0,len(y_test[0:100]),1), y_test[0:100], 'o--', color='black', alpha=0.5,label='validation')
plt.plot(np.arange(0,len(y_test[0:100]),1),reg.predict(X_test)[0:100], 'o--', color='green', alpha=0.5, label='prediction')
plt.ylabel('speed', fontsize=20)
plt.xlabel('time_steps', fontsize=20)
plt.xticks(fontsize=20)
plt.yticks(fontsize=20)
plt.title("Linear-Regression: Follower CFM")
plt.legend(fontsize=20)
plt.savefig("LRR_bi_i2.jpg", dpi=550)
plt.show()

#####
#multi-layer perceptron
#######
TargetVariable=['sub_vx']
#bidirectional
Predictors=['lead_vx','follo_vx',	'headw_leader',	'headw_follo']
#leader
# Predictors=['lead_vx','headw_leader']

#input1
X=CF_data[Predictors].values
y=CF_data[TargetVariable].values

### Sandardization of data ###

PredictorScaler=StandardScaler()
TargetVarScaler=StandardScaler()

# Storing the fit object for later reference
PredictorScalerFit=PredictorScaler.fit(X)
TargetVarScalerFit=TargetVarScaler.fit(y)

# Generating the standardized values of X and y
X=PredictorScalerFit.transform(X)
y=TargetVarScalerFit.transform(y)

# Split the data into training and testing set

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

n=len(Predictors)

# create ANN model
model = Sequential()
 
# Defining the Input layer and FIRST hidden layer, both are same!
model.add(Dense(units=64, input_dim=n, kernel_initializer='normal', activation='linear'))
 
# Defining the Second layer of the model
# after the first layer we don't have to specify input_dim as keras configure it automatically
model.add(Dense(units=64, kernel_initializer='normal', activation='linear'))

model.add(Dense(units=64, kernel_initializer='normal', activation='linear'))
 
# The output neuron is a single fully connected node 
# Since we will be predicting a single number
model.add(Dense(1, kernel_initializer='normal'))
 
# Compiling the model
model.compile(loss='mean_squared_error', optimizer='adam')
# plot_model(model, to_file='MLP_bi.jpeg')

model.fit(X_train, y_train ,batch_size = 10, epochs = 30, verbose=0)
 
# Generating Predictions on testing data
Predictions=model.predict(X_test)
 
# Scaling the predicted data back to original scale
Predictions=TargetVarScalerFit.inverse_transform(Predictions)
 
# Scaling the y_test data back to original scale
y_test_orig=TargetVarScalerFit.inverse_transform(y_test)
 
# Scaling the test data back to original scale
Test_Data=PredictorScalerFit.inverse_transform(X_test)

TestingData=pd.DataFrame(data=Test_Data, columns=Predictors)
TestingData['sub_vx']=y_test_orig
TestingData['Predicted_vx']=Predictions
TestingData.head()

TestingData.to_csv("MLP_ccc.csv")

# Computing the absolute percent error
APE=100*(abs(TestingData['sub_vx']-TestingData['Predicted_vx'])/TestingData['sub_vx'])
TestingData['APE']=abs(APE)
 
print('The Accuracy of ANN model is:', 100-np.mean(APE))
TestingData.head()

plt.figure(figsize=(15,6)) 
plt.plot(np.arange(0,len(TestingData['sub_vx'][0:100]),1), TestingData['sub_vx'][0:100], 'o--', color='black', alpha=0.5,label='validation')
plt.plot(np.arange(0,len(TestingData['sub_vx'][0:100]),1), TestingData['Predicted_vx'][0:100], 'o--', color='green', alpha=0.5, label='prediction')
plt.ylabel('speed', fontsize=20)
plt.xlabel('time_steps', fontsize=20)
plt.xticks(fontsize=20)
plt.yticks(fontsize=20)
plt.title("MLP-3-layer & 64-nodes:Follow")
plt.legend()
plt.savefig("ANN_bi_i2.jpg", dpi=550)
plt.show()

# RMSE (Root Mean Square Error)
from sklearn.metrics import mean_squared_error
from math import sqrt
rmse = float(format(np.sqrt(mean_squared_error(y_test_orig, Predictions)),'.3f'))
print("\nRMSE:\n",rmse)
print('Mean Absolute Error:', metrics.mean_absolute_error(y_test_orig,Predictions))
print('Mean Squared Error:', metrics.mean_squared_error(y_test_orig, Predictions))
print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test_orig,Predictions)))
print("Coefficient of determination: %.2f" % r2_score(y_test_orig, Predictions))

#########
#Random_Forest
#########
# create regressor object
# rfc = RandomForestRegressor(n_estimators = 100, random_state=123)
rfc = RandomForestRegressor(n_estimators=100, min_samples_split=6, min_samples_leaf=3, max_features='sqrt', max_depth=40, bootstrap=False)
# Fitting the Random Forest Regression model to the data
rfc.fit(X_train, y_train) 

# Predicting the target values of the test set
Predictions = rfc.predict(X_test)

# RMSE (Root Mean Square Error)
rmse = float(format(np.sqrt(mean_squared_error(y_test, Predictions)),'.3f'))

print("\nRMSE:\n",rmse)

plt.figure(figsize=(15,6)) 
plt.plot(np.arange(0,len(y_test[0:100]),1), y_test[0:100], 'o--', color='black', alpha=0.5,label='validation')
plt.plot(np.arange(0,len(y_test[0:100]),1), Predictions[0:100], 'o--', color='green', alpha=0.5,label='prediction')
plt.ylabel('speed', fontsize=20)
plt.xlabel('time_steps', fontsize=20)
plt.xticks(fontsize=20)
plt.yticks(fontsize=20)
plt.title("Random forest: follow")
plt.legend()
plt.savefig("RF_bi_i2.jpg", dpi=550)
plt.show()

# RMSE (Root Mean Square Error)
rmse = float(format(np.sqrt(mean_squared_error(y_test, Predictions)),'.3f'))
# Computing the absolute percent error
APE=100*(abs(y_test-Predictions)/y_test)
 
print('The Accuracy of ANN model is:', 100-np.mean(APE))
print("\nRMSE:\n",rmse)
print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, Predictions))
print('Mean Squared Error:', metrics.mean_squared_error(y_test,Predictions))
print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, Predictions)))
print("Coefficient of determination: %.2f" % r2_score(y_test_orig, Predictions))

CF_data.to_csv('input2.csv')

########
#multi-branch NN
#######
from sklearn.model_selection import train_test_split

CF_train = CF_data[CF_data['veh_no'] <= 3162]
CF_test = CF_data[CF_data['veh_no'] > 3162]
#network_archi
speed_x = Input(shape=(2,))
headway = Input(shape=(2,))

# the first branch operates on the first input
x = Dense(32, activation="linear")(speed_x)
x = Dense(16, activation="linear")(x)
x = Model(inputs=speed_x, outputs=x)
# the second branch opreates on the second input
y = Dense(32, activation="linear")(headway)
y = Dense(32, activation="linear")(y)
# y = Dense(20, activation="ReLU")(y)
y = Model(inputs=headway, outputs=y)
# combine the output of the two branches
combined = concatenate([x.output, y.output])
# apply a FC layer and then a regression prediction on the
# combined outputs
z = Dense(3, activation="linear")(combined)
z = Dense(1, activation="linear")(z)
# our model will accept the inputs of the two branches and
# then output a single value
model = Model(inputs=[x.input, y.input], outputs=z)
plot_model(model, to_file='multi_branch_bi.jpeg')
model.summary()

#model_fit_bileaner
model.compile(loss='mean_squared_error', optimizer='adam')

model.fit([CF_train[['lead_vx','follo_vx']], CF_train[['headw_leader','headw_follo']]], CF_train[['sub_vx']],batch_size = 5, epochs = 15, verbose=0)
# Generating Predictions on testing data
Predictions=model.predict([CF_test[['lead_vx','follo_vx']], CF_test[['headw_leader','headw_follo']]])
 
# Scaling the predicted data back to original scale
# Predictions=TargetVarScalerFit.inverse_transform(Predictions)
 
# Scaling the y_test data back to original scale
y_test_orig=CF_test[['sub_vx']]
 
# Scaling the test data back to original scale
# Test_Data=PredictorScalerFit.inverse_transform(CF_test[['lead_vx','follo_vx']])

Test_Data= CF_test[['lead_vx','follo_vx']]
TestingData=pd.DataFrame(data=Test_Data, columns=['lead_vx','follo_vx'])
TestingData['sub_vx']=y_test_orig
TestingData['Predicted_vx']=Predictions
TestingData

plt.figure(figsize=(15,6)) 
plt.plot(np.arange(0,len(TestingData['sub_vx'][2:100]),1), TestingData['sub_vx'][2:100], 'o--', color='black', alpha=0.5,label='validation')
plt.plot(np.arange(0,len(TestingData['sub_vx'][2:100]),1), TestingData['Predicted_vx'][2:100], 'o--', color='green', alpha=0.5, label='prediction')
plt.ylabel('speed', fontsize=20)
plt.xlabel('time_steps', fontsize=20)
plt.xticks(fontsize=20)
plt.yticks(fontsize=20)
plt.title("Multi-branch NN: follw")
plt.legend()
plt.savefig("multibranch_bi_i2.jpg", dpi=550)
plt.show()

# Computing the absolute percent error
APE=100*(abs(TestingData['sub_vx']-TestingData['Predicted_vx'])/TestingData['sub_vx'])
TestingData['APE']=abs(APE)
 
print('The Accuracy of ANN model is:', 100-np.mean(APE))
TestingData.head()

# RMSE (Root Mean Square Error)
from sklearn.metrics import mean_squared_error
from math import sqrt
rmse = float(format(np.sqrt(mean_squared_error(y_test_orig, Predictions)),'.3f'))
print("\nRMSE:\n",rmse)
print('Mean Absolute Error:', metrics.mean_absolute_error(y_test_orig,Predictions))
print('Mean Squared Error:', metrics.mean_squared_error(y_test_orig, Predictions))
print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test_orig,Predictions)))
print("Coefficient of determination: %.2f" % r2_score(y_test_orig, Predictions))

import seaborn as sns
plt.figure(figsize= (12,10))
cor =X_train.corr()
sns.heatmap(cor, annot=True, annot_kws={"size":20}, cmap= plt.cm.CMRmap_r)
sns.set(font_scale=2)
# plt.savefig("corr_car_tw_tr.jpg", dpi=550)
plt.show

plt.figure(figsize= (12,10))
with sns.plotting_context(rc={"axes.labelsize":30}):
    sns.heatmap(cor, annot=True, cmap= plt.cm.CMRmap_r)
    # sns.pairplot(new, corner=True)

